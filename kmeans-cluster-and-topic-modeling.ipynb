{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5a18aefa-1518-4d43-abc2-b1fa9051d7b5",
    "_uuid": "cdbef80edb46e7f336491a700dce38e074398784",
    "colab_type": "text",
    "id": "_QpbURobkj6S"
   },
   "source": [
    "# Introduction\n",
    "In this kernal, I will use TF-IDF to vectorize the [articles data](https://www.kaggle.com/snapcrack/all-the-news/data) and cluster them. \n",
    "Then, I will make paper recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c94c0f3d-5b5b-42f5-b4c9-5ddc2c3c5af1",
    "_uuid": "cfb448e8b9beb8d8a857da15376730dd8d0c628f",
    "colab_type": "text",
    "id": "J2xhwGlXkj6V"
   },
   "source": [
    "**1. Input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "ae1fbc7a-e384-4199-8f36-f1d1447bf786",
    "_uuid": "947abb51738a2c42e9bd1b465720bd624605ea06",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2029,
     "status": "error",
     "timestamp": 1550396822197,
     "user": {
      "displayName": "Ashish Salunkhe",
      "photoUrl": "https://lh5.googleusercontent.com/-JA8wS1A42o8/AAAAAAAAAAI/AAAAAAAACpw/8iNsa5IGzj8/s64/photo.jpg",
      "userId": "03884226030778664360"
     },
     "user_tz": -330
    },
    "id": "iUmEv-MVkj6X",
    "outputId": "1d5f8d06-d29d-4c37-d92e-07293bf71ddc"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../input/articles1.csv does not exist: '../input/articles1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-add2b961ff74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/articles1.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ../input/articles1.csv does not exist: '../input/articles1.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = pd.read_csv('../input/articles1.csv')\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d997a92-0d16-41dc-89b8-6901b535af17",
    "_uuid": "07466665856dc0b0a0d89d7e50adf4da423ca315",
    "colab_type": "text",
    "id": "k69Ad4J-kj6e"
   },
   "source": [
    "**2. Using TF-IDF vectorize the articles**\n",
    "\n",
    "[Introduction](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "ee7442ad-89da-493c-959f-2d0b7e3c9a40",
    "_uuid": "c43305078269683f780d6b2cf8c35efd20f5f9d4",
    "colab": {},
    "colab_type": "code",
    "id": "_OCiMGe-kj6f"
   },
   "outputs": [],
   "source": [
    "text_content = text['content']\n",
    "vector = TfidfVectorizer(stop_words = 'english')\n",
    "tfidf = vector.fit_transform(text_content)\n",
    "text_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1ea7f69-3851-447e-8c2c-cbd3c474b15d",
    "_uuid": "f07b0f508af44fe7df95ec2b8540745d4db61dd6",
    "colab_type": "text",
    "id": "g03j2vfikj6i"
   },
   "source": [
    "**3. Using elbow method to decide cluster number**\n",
    "\n",
    "Introduction to [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "3j5Zg2ESkj6k"
   },
   "outputs": [],
   "source": [
    "K = range(1,15)\n",
    "SSE = []\n",
    "for k in K:\n",
    "    kmeans = MiniBatchKMeans(n_clusters = k,batch_size = 300)\n",
    "    kmeans.fit(tfidf)\n",
    "    SSE.append(kmeans.inertia_)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(K,SSE,'bx-')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('cluster numbers')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4aa9e2e3-7f2a-42e1-ad83-1554af72222a",
    "_uuid": "ff2c3b75655ac0b3328820d7adb7414079e5a8ca",
    "colab_type": "text",
    "id": "Qj1KNoUfkj6p"
   },
   "source": [
    "**5. Using MiniBatchKMean to cluster**\n",
    "\n",
    "[Comparison of the K-Means and MiniBatchKMeans clustering algorithms](http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "zDg1TAymkj6q"
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "kmeans = MiniBatchKMeans(n_clusters = k)\n",
    "kmeans.fit(tfidf)\n",
    "centers = kmeans.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vector.get_feature_names()\n",
    "\n",
    "for i in range(0,k):\n",
    "    word_list=[]\n",
    "    print(\"cluster%d:\"% i)\n",
    "    for j in centers[i,:15]:\n",
    "        word_list.append(terms[j])\n",
    "    print(word_list) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b372fe7e-022d-4c9a-8e57-573cbccb7a64",
    "_uuid": "13f971873eb2d912b5e1189a8592bf2be97c2838",
    "colab_type": "text",
    "id": "KAhgpKwkkj6v"
   },
   "source": [
    "**6. Article Recommendation**\n",
    "\n",
    "Since we had vectorize the articles by TF-IDF, we only need to compare articles by comparing their vectors. Bigger inner product of two vectors means they shares more similar information. Here we take the first article in artciles3.csv--Alton Sterling’s son: ’Everyone needs to protest the right way, with peace’ as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "af9ec5de-0378-4e74-97f2-76c5829559a1",
    "_uuid": "9236750bbf075652b5cb03ed4a88a84c6f991ac2",
    "colab": {},
    "colab_type": "code",
    "id": "9IKd5dNkkj6w"
   },
   "outputs": [],
   "source": [
    "similarity = np.dot(tfidf[0],np.transpose(tfidf))\n",
    "x = np.array(similarity.toarray()[0])\n",
    "print(text['title'][0])\n",
    "print('\\nsimiliar papers:')\n",
    "print('\\n'.join(text['title'].loc[np.argsort(x)[-7:-2]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "072d722c-169b-4720-8891-460d31fa95e2",
    "_uuid": "9f53f5a90394a45fa50610d65c2c718ae151cbf9",
    "colab_type": "text",
    "id": "WBCBJV_Bkj61"
   },
   "source": [
    "**7. Topic modeling **\n",
    "\n",
    "Here I use NMF instead of LDA because LDA's calculation is time-consuming. \n",
    "\n",
    "The major topics consist of politics, society, finance and internetional affairs, which is quite similiar to our cluster resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "5475b769-2ed7-4d15-9283-802d1a28cf98",
    "_uuid": "841f54807d352edb5e940e51c49d7e3614a27814",
    "colab": {},
    "colab_type": "code",
    "id": "DVvVoysNkj62"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf  = NMF(n_components = 4)\n",
    "nmf.fit(tfidf)\n",
    "for i in range(0,k):\n",
    "    word_list=[]\n",
    "    print(\"Topic%d:\"% i)\n",
    "    for j in nmf.components_.argsort()[i,-16:-1]:\n",
    "        word_list.append(terms[j])\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28832dc3-4b5f-416c-b9df-fe2130a1358c",
    "_uuid": "0768e23df87e5b0ba89f1e070331689d90356e92",
    "colab_type": "text",
    "id": "fNZqqEuGkj66"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "kmeans-cluster-and-topic-modeling.xpynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
